[pytest]
# Medical AI Assistant - Integration Testing Framework Configuration
# Test Framework Version: 1.0.0
# Last Updated: 2025-11-04

# Test Discovery
testpaths = testing/integration
python_files = test_*.py *_test.py
python_classes = Test* *Tests
python_functions = test_*

# Async Configuration
asyncio_mode = auto
asyncio_default_loop_scope = test
addopts = --strict-markers --strict-config --verbose --tb=short --show-capture=all

# Output and Formatting
console_output_style = progress
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests/log/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d: %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Minimum Requirements
minversion = 6.0
required_plugins = pytest-asyncio>=0.18.0

# Markers (Custom Test Categories)
markers =
    # Test Categories
    unit: marks tests as unit tests (deselect with '-m "not unit"')
    integration: marks tests as integration tests
    e2e: marks tests as end-to-end tests
    performance: marks tests as performance benchmarks
    reliability: marks tests as system reliability and recovery
    websocket: marks tests as WebSocket communication tests
    patient_flow: marks tests as patient chat workflow tests
    nurse_workflow: marks tests as nurse dashboard workflow tests
    training_serving: marks tests as training-serving integration tests
    
    # Test Priority
    critical: marks tests as critical path tests (highest priority)
    smoke: marks tests as smoke tests (basic functionality)
    regression: marks tests as regression tests
    
    # Test Execution
    slow: marks tests as slow running (>30 seconds)
    fast: marks tests as fast running (<5 seconds)
    concurrent: marks tests that require concurrent execution
    parallel: marks tests that can run in parallel
    
    # Environment
    local: marks tests that run in local development
    ci: marks tests that run in CI/CD environments
    staging: marks tests that run in staging environment
    production: marks tests that run in production monitoring
    
    # Data
    mock_data: marks tests that use mock data
    real_data: marks tests that use real data
    synthetic_data: marks tests that use synthetic generated data

# Test Execution
timeout = 300
timeout_method = thread
maxfail = 5
xfail_strict = true

# Parallel Execution (with pytest-xdist)
addopts = -n auto

# Coverage Configuration (with pytest-cov)
addopts = --cov=src --cov-report=term-missing --cov-report=html --cov-report=xml
omit =
    */tests/*
    */test_*
    setup.py
    conftest.py

# Filter Warnings
filterwarnings =
    error
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore:.*unclosed.*:ResourceWarning
    ignore:.*metadata.*:DeprecationWarning
    ignore:.*ssl.*:DeprecationWarning
    ignore:.*timeout.*:DeprecationWarning

# JUnit XML Report
junit_suite_name = medical_ai_integration_tests
junit_logging = all
junit_log_passing_tests = true
junit_duration_report = total
junit_family = xunit2

# Custom Arguments for Different Scenarios
# For quick smoke tests:
# pytest -m smoke -v

# For performance tests only:
# pytest -m performance --benchmark-only

# For critical path tests:
# pytest -m critical -v

# For all tests except slow ones:
# pytest -m "not slow" -v

# For local development (verbose output):
# pytest -v -s --log-cli-level=DEBUG

# For CI/CD (minimal output, XML reports):
# pytest --tb=short --maxfail=1 --junitxml=test-results.xml

# For debugging specific test:
# pytest -v -s --log-cli-level=DEBUG test_specific_module.py::test_specific_function

# Environment Variables
env = 
    PYTHONPATH = .
    TESTING = true
    ASYNC_TEST_TIMEOUT = 300
    MAX_CONCURRENT_TESTS = 4
    PERFORMANCE_THRESHOLD = 2.5
    RELIABILITY_THRESHOLD = 0.99