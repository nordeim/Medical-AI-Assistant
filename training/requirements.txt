# DeepSpeed Distributed Training Requirements

# Core DeepSpeed and PyTorch
deepspeed>=0.12.0
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# Hugging Face Transformers (for model loading)
transformers>=4.30.0
tokenizers>=0.13.0

# Datasets and data loading
datasets>=2.12.0
torchvision>=0.15.0
torchaudio>=2.0.0

# Scientific computing
numpy>=1.24.0
scipy>=1.10.0
pandas>=2.0.0

# System monitoring and profiling
psutil>=5.9.0
nvidia-ml-py3>=7.352.0
memory-profiler>=0.61.0

# Progress bars and utilities
tqdm>=4.65.0
click>=8.1.0
colorama>=0.4.6

# Configuration and serialization
pyyaml>=6.0
configparser>=5.3.0

# Logging and monitoring
tensorboard>=2.13.0
wandb>=0.15.0

# Distributed training utilities
huggingface-hub>=0.15.0
accelerate>=0.20.0

# Development and testing utilities
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.0.0
flake8>=6.0.0
mypy>=1.4.0

# Optional: For advanced model loading
sentencepiece>=0.1.99
protobuf>=4.21.0

# Optional: For data preprocessing
scikit-learn>=1.3.0

# Optional: For specific model architectures
timm>=0.9.0

# Optional: For advanced optimization
apex>=0.1.0  # For mixed precision training optimization (NVIDIA)
fairscale>=0.4.0  # Alternative parallelization library

# System-specific requirements (install manually if needed)
# NCCL: Should be installed with CUDA
# OpenMPI: For multi-node training (system install)
# InfiniBand drivers: For high-performance networking (system install)