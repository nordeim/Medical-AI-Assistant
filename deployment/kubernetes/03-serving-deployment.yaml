# Model Serving Production Deployment for Kubernetes
# GPU-enabled deployment for medical AI inference with compliance
apiVersion: apps/v1
kind: Deployment
metadata:
  name: medical-ai-serving
  namespace: medical-ai-prod
  labels:
    app: medical-ai
    component: serving
    environment: production
    compliance: hipaa
    accelerator: nvidia-tesla-k80
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  selector:
    matchLabels:
      app: medical-ai
      component: serving
  template:
    metadata:
      labels:
        app: medical-ai
        component: serving
        environment: production
        compliance: hipaa
        accelerator: nvidia-tesla-k80
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8081"
        prometheus.io/path: "/metrics"
        audit.kubernetes.io/allow-privilege-escalation: "false"
        audit.kubernetes.io/readonly-root-filesystem: "true"
    spec:
      serviceAccountName: medical-ai-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        runAsGroup: 1001
        fsGroup: 1001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
      - name: wait-for-backend
        image: medical-ai/backend:latest
        command:
        - sh
        - -c
        - |
          until curl -f http://medical-ai-backend:8000/health; do
            echo 'Waiting for backend...'
            sleep 2
          done
          echo 'Backend is ready!'
      - name: nvidia-device-plugin-ctr
        image: nvcr.io/nvidia/k8s-device-plugin:v0.14.0
        command: ["nvidia-smi"]
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
      containers:
      - name: serving
        image: medical-ai/serving:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8081
          name: http
          protocol: TCP
        - containerPort: 9090
          name: grpc
          protocol: TCP
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: OMP_NUM_THREADS
          value: "1"
        - name: TOKENIZERS_PARALLELISM
          value: "false"
        - name: CUDA_MEMORY_FRACTION
          value: "0.8"
        - name: MODEL_CACHE_SIZE
          value: "50"
        - name: MAX_BATCH_SIZE
          value: "32"
        - name: MAX_SEQUENCE_LENGTH
          value: "512"
        - name: INFERENCE_TIMEOUT
          value: "300"
        - name: ENABLE_METRICS
          value: "true"
        - name: PROMETHEUS_MULTIPROC_DIR
          value: "/tmp/prometheus_multiproc_dir"
        resources:
          requests:
            nvidia.com/gpu: 1
            cpu: 1000m
            memory: 4Gi
          limits:
            nvidia.com/gpu: 1
            cpu: 4
            memory: 16Gi
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1001
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: models
          mountPath: /app/models
        - name: cache
          mountPath: /app/cache
        - name: logs
          mountPath: /app/logs
        - name: prometheus-multiproc
          mountPath: /tmp/prometheus_multiproc_dir
        - name: nvidia-tmp
          mountPath: /tmp
          readOnly: false
          mountPropagation: HostToContainer
        - name: nvidia-driver
          mountPath: /usr/local/nvidia
          readOnly: true
        livenessProbe:
          exec:
            command:
            - nvidia-smi
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8081
          initialDelaySeconds: 30
          periodSeconds: 15
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /health
            port: 8081
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 30
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: medical-ai-models-pvc
      - name: cache
        emptyDir:
          medium: Memory
          sizeLimit: 2Gi
      - name: logs
        emptyDir: {}
      - name: prometheus-multiproc
        emptyDir: {}
      - name: nvidia-tmp
        emptyDir: {}
      - name: nvidia-driver
        hostPath:
          path: /usr/local/nvidia
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - medical-ai
                - key: component
                  operator: In
                  values:
                  - serving
              topologyKey: kubernetes.io/hostname
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: accelerator
                operator: In
                values:
                - nvidia-tesla-k80
                - nvidia-tesla-v100
                - nvidia-tesla-p100
                - nvidia-tesla-p4
                - nvidia-tesla-t4
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "medical-ai-dedicated"
        operator: "Equal"
        value: "serving"
        effect: "NoSchedule"
      nodeSelector:
        accelerator: nvidia-tesla-k80
        workload-type: gpu
---
# Service for Model Serving
apiVersion: v1
kind: Service
metadata:
  name: medical-ai-serving
  namespace: medical-ai-prod
  labels:
    app: medical-ai
    component: serving
    environment: production
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
spec:
  type: ClusterIP
  ports:
  - port: 8081
    targetPort: 8081
    protocol: TCP
    name: http
  - port: 9090
    targetPort: 9090
    protocol: TCP
    name: grpc
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
  selector:
    app: medical-ai
    component: serving
---
# HorizontalPodAutoscaler for Model Serving
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: medical-ai-serving-hpa
  namespace: medical-ai-prod
  labels:
    app: medical-ai
    component: serving
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: medical-ai-serving
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Resource
    resource:
      name: nvidia.com/gpu
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: inference_requests_per_second
      target:
        type: AverageValue
        averageValue: "20"
  - type: Pods
    pods:
      metric:
        name: average_inference_time
      target:
        type: AverageValue
        averageValue: "2"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 25
        periodSeconds: 120
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 120
      - type: Pods
        value: 1
        periodSeconds: 120
---
# PodDisruptionBudget for serving
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: medical-ai-serving-pdb
  namespace: medical-ai-prod
  labels:
    app: medical-ai
    component: serving
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: medical-ai
      component: serving
---
# GPU Resource Quota (if using GPU quotas)
apiVersion: v1
kind: ResourceQuota
metadata:
  name: medical-ai-gpu-quota
  namespace: medical-ai-prod
spec:
  hard:
    requests.nvidia.com/gpu: "4"
    limits.nvidia.com/gpu: "4"