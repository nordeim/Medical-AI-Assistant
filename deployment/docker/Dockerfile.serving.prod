# Production Dockerfile for Model Serving - Optimized for inference workloads
# Stage 1: Build stage
FROM nvidia/cuda:11.8-devel-ubuntu20.04 AS builder

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.9 \
    python3.9-pip \
    python3.9-dev \
    build-essential \
    curl \
    wget \
    git \
    && rm -rf /var/lib/apt/lists/*

# Create Python symlink
RUN ln -sf /usr/bin/python3.9 /usr/bin/python

# Install pip for Python 3.9
RUN curl -sS https://bootstrap.pypa.io/get-pip.py | python

# Create virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy requirements
COPY serving/requirements.txt ./
COPY training/requirements.txt ./training_requirements.txt

# Install Python dependencies with CUDA support
RUN pip install --upgrade pip && \
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 && \
    pip install -r requirements.txt && \
    pip install -r training_requirements.txt

# Stage 2: Production stage
FROM nvidia/cuda:11.8-runtime-ubuntu20.04 AS production

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    curl \
    ca-certificates \
    && apt-get upgrade -y \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create non-root user
RUN groupadd -r appgroup && useradd -r -g appgroup appuser

# Copy virtual environment from builder
COPY --from=builder /opt/venv /opt/venv

# Set environment variables
ENV PATH="/opt/venv/bin:$PATH" \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    CUDA_VISIBLE_DEVICES=0 \
    OMP_NUM_THREADS=1 \
    TOKENIZERS_PARALLELISM=false

# Set working directory
WORKDIR /app

# Copy application code
COPY --chown=appuser:appgroup serving/ ./serving/
COPY --chown=appuser:appgroup training/ ./training/

# Create necessary directories
RUN mkdir -p /app/logs /app/data /app/models /app/cache /tmp && \
    chown -R appuser:appgroup /app

# Performance optimizations
# Set CUDA memory fraction
ENV CUDA_MEMORY_FRACTION=0.8

# Enable MPSL (Multi-Process Service Layer) for better GPU utilization
ENV NVIDIA_TDR_DEFAULT=vgpu

# Switch to non-root user
USER appuser

# Expose port
EXPOSE 8081

# Health check for GPU availability
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD nvidia-smi > /dev/null 2>&1 || exit 1

# Start command
CMD ["python", "-m", "serving.main", "--host", "0.0.0.0", "--port", "8081"]