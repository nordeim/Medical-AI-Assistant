# Medical AI Assistant Training Environment Configuration
# Copy this file to .env and update the values according to your setup

# ============================
# Model and Data Paths
# ============================

# Base path for all model-related files
MODEL_BASE_PATH=/workspace/Medical-AI-Assistant/models

# Path to processed training data
TRAINING_DATA_PATH=/workspace/Medical-AI-Assistant/training/data

# Output directory for training results and checkpoints
OUTPUT_DIR=/workspace/Medical-AI-Assistant/training/outputs

# Directory for model checkpoints during training
CHECKPOINT_DIR=/workspace/Medical-AI-Assistant/training/checkpoints

# Directory for final trained models
MODEL_REGISTRY_DIR=/workspace/Medical-AI-Assistant/training/models

# ============================
# Model Configuration
# ============================

# Base model identifier from Hugging Face Hub
MODEL_NAME=microsoft/DialoGPT-medium
# MODEL_NAME=meta-llama/Llama-2-7b-hf
# MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.1

# Model type for training
# Options: causal_lm, seq2seq_lm, seq2seq
MODEL_TYPE=causal_lm

# Maximum sequence length for training
MAX_SEQ_LENGTH=512

# Training batch size (adjust based on GPU memory)
BATCH_SIZE=4
# GRADIENT_ACCUMULATION_STEPS=4

# Learning rate for training
LEARNING_RATE=5e-5

# Number of training epochs
NUM_EPOCHS=3

# ============================
# Training Hardware Configuration
# ============================

# CUDA device configuration
CUDA_VISIBLE_DEVICES=0

# Number of GPUs available for training
N_GPUS=1

# Distributed training configuration
MASTER_ADDR=localhost
MASTER_PORT=29500

# Ranks for distributed training (if using multiple GPUs/nodes)
RANK=0
LOCAL_RANK=0
WORLD_SIZE=1

# ============================
# Optimization and Performance
# ============================

# Enable mixed precision training (fp16)
FP16=true

# Use gradient checkpointing to save memory
GRADIENT_CHECKPOINTING=true

# Enable DeepSpeed for distributed training
DEEPSPEED_CONFIG_PATH=/workspace/Medical-AI-Assistant/training/configs/deepspeed_config.json

# Enable CPU offloading with DeepSpeed
CPU_OFFLOAD=false

# Enable activation checkpointing
ACTIVATION_CHECKPOINTING=true

# ============================
# Experiment Tracking
# ============================

# Weights & Biases configuration
WANDB_PROJECT=medical-ai-assistant
WANDB_RUN_NAME=clinic-training-$(date +%Y%m%d-%H%M%S)
WANDB_ENTITY=your-wandb-username
WANDB_API_KEY=your-wandb-api-key

# Enable TensorBoard logging
TENSORBOARD_LOG_DIR=/workspace/Medical-AI-Assistant/training/logs/tensorboard

# ============================
# Hugging Face Hub Configuration
# ============================

# Hugging Face API token for model downloading/uploading
HUGGINGFACE_HUB_TOKEN=hf_your-huggingface-token-here

# Local cache directory for Hugging Face models and datasets
HF_HOME=/workspace/Medical-AI-Assistant/.cache/huggingface

# ============================
# Data Processing Configuration
# ============================

# Data preprocessing parameters
DATA_MAX_SAMPLES=10000  # Limit for initial testing
DATA_VALIDATION_SPLIT=0.1
DATA_TEST_SPLIT=0.1
DATA_SEED=42

# Text preprocessing settings
MAX_TEXT_LENGTH=1024
MIN_TEXT_LENGTH=10

# ============================
# Training Loop Configuration
# ============================

# Logging and evaluation frequency
LOGGING_STEPS=100
EVALUATION_STEPS=500
SAVE_STEPS=1000
SAVE_TOTAL_LIMIT=3

# Early stopping configuration
EARLY_STOPPING_PATIENCE=3
EARLY_STOPPING_THRESHOLD=0.01

# ============================
# LoRA and Fine-tuning Configuration
# ============================

# Enable LoRA (Low-Rank Adaptation) for efficient fine-tuning
USE_LORA=true

# LoRA parameters
LORA_R=16  # Rank of LoRA matrices
LORA_ALPHA=32  # Scaling parameter
LORA_DROPOUT=0.1  # Dropout rate for LoRA
LORA_TARGET_MODULES=attn,mlp  # Modules to apply LoRA to

# ============================
# Quantization Configuration
# ============================

# Enable 4-bit quantization for memory efficiency
USE_4BIT=true

# BitsAndBytes configuration
BITSANDBYTES_4BIT_COMPUTE_DTYPE=float16
BITSANDBYTES_4BIT_QUANT_TYPE=nf4
BITSANDBYTES_4BIT_USE_DOUBLE_QUANT=false

# ============================
# Training Safety and Validation
# ============================

# Enable input validation and sanitization
ENABLE_INPUT_VALIDATION=true

# Maximum input sequence length for validation
MAX_VALIDATION_SEQ_LENGTH=2048

# Enable response filtering
ENABLE_RESPONSE_FILTERING=true

# Safety thresholds
MAX_RESPONSE_LENGTH=512
MAX_TOKENS_PER_REQUEST=2048

# ============================
# Debugging and Development
# ============================

# Enable debug mode for verbose logging
DEBUG_MODE=false

# Enable profiling for performance analysis
PROFILING_ENABLED=false

# Random seeds for reproducibility
PYTHONHASHSEED=42
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# ============================
# Cloud and Storage Configuration
# ============================

# Cloud storage settings (if using cloud storage)
CLOUD_PROVIDER=local  # Options: local, aws, gcp, azure
S3_BUCKET_NAME=your-s3-bucket
S3_REGION=us-east-1

# Backup configuration
BACKUP_ENABLED=true
BACKUP_FREQUENCY=daily
BACKUP_RETENTION_DAYS=30

# ============================
# Resource Monitoring
# ============================

# Memory management
MEMORY_FRACTION=0.8  # Fraction of GPU memory to use
ENABLE_MEMORY_OPTIMIZATION=true

# Disk space monitoring
MIN_DISK_SPACE_GB=10  # Minimum free disk space required

# Network timeout settings
HTTP_TIMEOUT=300
DOWNLOAD_TIMEOUT=600

# ============================
# Security Configuration
# ============================

# API security settings
API_RATE_LIMIT=1000
ENABLE_API_AUTH=true

# Data encryption settings
ENCRYPTION_ENABLED=false
ENCRYPTION_KEY=your-encryption-key-here

# ============================
# Version Control and Reproducibility
# ============================

# Training configuration versioning
CONFIG_VERSION=1.0
EXPERIMENT_ID=exp-$(date +%Y%m%d-%H%M%S)

# Git configuration for reproducibility
GIT_COMMIT_HASH=$(git rev-parse HEAD 2>/dev/null || echo "unknown")
GIT_BRANCH=$(git branch --show-current 2>/dev/null || echo "unknown")

# ============================
# Development and Testing
# ============================

# Training data subset for quick testing
QUICK_TEST_MODE=false
QUICK_TEST_SAMPLES=100

# Automated testing configuration
AUTO_TEST_ENABLED=false
TEST_CHECKPOINT_EVERY_STEPS=500

# Performance benchmarking
BENCHMARK_MODE=false
BENCHMARK_SAMPLES=1000